{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p align=\"center\" style=\"color:rgb(55, 113, 197);\">[16-825: Learning for 3D Vision](https://learning3d.github.io/)</p>\n",
    "### <p align=\"center\" style=\"color:rgb(0, 0, 0);\">Point Clouds</p>\n",
    "<p align=\"center\" style=\"color:rgb(55, 113, 197);\">\n",
    "    Shreyas Jha &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "    <a href=\"https://github.com/Shreyas120/l3d_assignment5\">Code</a> \n",
    "</p>\n",
    "\n",
    "<!-- Color theme\n",
    "    Dark color:rgb(42, 88, 153)\n",
    "    Medium (55, 113, 197)\n",
    "    Light and bright color:rgb(68, 129, 211)\n",
    " -->\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p align=\"center\" style=\"color:rgb(55, 113, 197);\">1.&nbsp;Classification Model </p>\n",
    "\n",
    "\n",
    "### <p align=\"center\" style=\"color:rgb(55, 113, 197);\">Test accuracy of best model = 96.53%</p>\n",
    "\n",
    "### <p align=\"center\" style=\"color:rgb(55, 113, 197);\">True Positives</p>\n",
    "\n",
    "#### <p align=\"center\" style=\"color:rgb(55, 113, 197);\">Predicted Chair </p>\n",
    "#### <p align=\"center\" style=\"color:rgb(42, 88, 153);\">![GIF](./output/cls/pred0_gt0_TP354.gif)</p>\n",
    "\n",
    "#### <p align=\"center\" style=\"color:rgb(55, 113, 197);\">Predicted Chair</p>\n",
    "#### <p align=\"center\" style=\"color:rgb(42, 88, 153);\">![GIF](./output/cls/pred0_gt0_TP417.gif)</p>\n",
    "\n",
    "#### <p align=\"center\" style=\"color:rgb(55, 113, 197);\">Predicted Lamp</p>\n",
    "#### <p align=\"center\" style=\"color:rgb(42, 88, 153);\">![GIF](./output/cls/pred2_gt2_TP815.gif)</p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### <p align=\"center\" style=\"color:rgb(55, 113, 197);\">False Positives</p>\n",
    "\n",
    "#### <p align=\"center\" style=\"color:rgb(55, 113, 197);\">Failure: Predicted chair as lamp </p>\n",
    "#### <p align=\"center\" style=\"color:rgb(42, 88, 153);\">![GIF](./output/cls/pred2_gt0_FP36.gif)</p>\n",
    "\n",
    "#### <p align=\"center\" style=\"color:rgb(55, 113, 197);\">Failure: Predicted lamp as vase </p>\n",
    "#### <p align=\"center\" style=\"color:rgb(42, 88, 153);\">![GIF](./output/cls/pred1_gt2_FP719.gif)</p>\n",
    "\n",
    "#### <p align=\"center\" style=\"color:rgb(55, 113, 197);\">Failure: Predicted vase as lamp</p>\n",
    "#### <p align=\"center\" style=\"color:rgb(42, 88, 153);\">![GIF](./output/cls/pred2_gt1_FP647.gif)</p>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "The model accurately classifies chairs but confuses lamps and vases, likely due to similar structural features. The model struggles to differentiate between lamps and vases, as evidenced by the false positives. The chair mistaken for a lamp may be due to similar structural features that the model has learned to associate with lamps, such as protruding parts that could be misinterpreted as lamp arms. Similarly, the lamp mistaken for a vase suggests the model confuses certain shapes or topologies that are common between the two, such as a base with an extending part that could resemble a lampshade or the neck of a vase. The confusion matrix (shown later in the webpage) shows a high success rate for chairs but significant misclassifications between lamps and vases, indicating the model's difficulty in distinguishing between these items with comparable shapes.\n",
    "\n",
    "</dov>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p align=\"center\" style=\"color:rgb(55, 113, 197);\">2.&nbsp;Segmenation Model </p>\n",
    "\n",
    "\n",
    "### <p align=\"center\" style=\"color:rgb(55, 113, 197);\">Test accuracy of best model = 90.1%</p>\n",
    "\n",
    "### <p align=\"center\" style=\"color:rgb(55, 113, 197);\">Good Predictions</p>\n",
    "\n",
    "#### <p align=\"center\" style=\"color:rgb(55, 113, 197);\">Predicted Accuracy 99%</p>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "| Predicted | Ground Truth |\n",
    "|:--------:|:--------:|\n",
    "| ![GIF](./output/seg/pred_297_acc99.gif)    | ![GIF](./output/seg/gt_297.gif)     |\n",
    "\n",
    "</div>\n",
    "\n",
    "#### <p align=\"center\" style=\"color:rgb(55, 113, 197);\">Predicted Accuracy 99%</p>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "| Predicted | Ground Truth |\n",
    "|:--------:|:--------:|\n",
    "| ![GIF](./output/seg/pred_471_acc99.gif)    | ![GIF](./output/seg/gt_471.gif)     |\n",
    "\n",
    "</div>\n",
    "\n",
    "#### <p align=\"center\" style=\"color:rgb(55, 113, 197);\">Predicted Accuracy 99%</p>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "| Predicted | Ground Truth |\n",
    "|:--------:|:--------:|\n",
    "| ![GIF](./output/seg/pred_490_acc99.gif)    | ![GIF](./output/seg/gt_490.gif)     |\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "### <p align=\"center\" style=\"color:rgb(55, 113, 197);\">Bad Predictions</p>\n",
    "\n",
    "#### <p align=\"center\" style=\"color:rgb(55, 113, 197);\">Predicted Accuracy 41%</p>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "| Predicted | Ground Truth |\n",
    "|:--------:|:--------:|\n",
    "| ![GIF](./output/seg/pred_605_acc41.gif)    | ![GIF](./output/seg/gt_605.gif)     |\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "#### <p align=\"center\" style=\"color:rgb(55, 113, 197);\">Predicted Accuracy 47%</p>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "| Predicted | Ground Truth |\n",
    "|:--------:|:--------:|\n",
    "| ![GIF](./output/seg/pred_235_acc47.gif)    | ![GIF](./output/seg/gt_235.gif)     |\n",
    "\n",
    "</div>\n",
    "\n",
    "#### <p align=\"center\" style=\"color:rgb(55, 113, 197);\"></p>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "The provided results depict a point cloud segmentation model's performance in classifying parts of chairs. In the top image, where the model achieves 99% accuracy, it correctly differentiates between the chair's components, likely because they match well with its learned patterns from the training data. In the lower images with accuracies of 41% and 47%, the model confuses the sides of the chairs with the handles, indicating a challenge in distinguishing between similar geometric features or dealing with varying orientations and complex shapes not adequately represented in the training dataset. These inconsistencies suggest that this basic implementation might lack robustness or generalization capabilities for different viewpoints or more complex configurations.\n",
    "\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p align=\"center\" style=\"color:rgb(55, 113, 197);\">3.&nbsp;Robustness analysis </p>\n",
    "\n",
    "### <p align=\"center\" style=\"color:rgb(55, 113, 197);\">3.1&nbsp;Classification</p>\n",
    "\n",
    "#### <p align=\"center\" style=\"color:rgb(55, 113, 197);\">3.1.1&nbsp;Randomly rotating the input point cloud </p>\n",
    "#### <p align=\"center\" style=\"color:rgb(42, 88, 153);\">![GIF](./output/cls/accuracy_vs_rotation.png)</p>\n",
    "\n",
    "#### <p align=\"center\" style=\"color:rgb(55, 113, 197);\">3.1.2&nbsp;Varying the number of points in the input point cloud </p>\n",
    "#### <p align=\"center\" style=\"color:rgb(42, 88, 153);\">![GIF](./output/cls/accuracy_vs_npts.png)</p>\n",
    "\n",
    "\n",
    "### <p align=\"center\" style=\"color:rgb(55, 113, 197);\">3.2&nbsp;Segmentation</p>\n",
    "\n",
    "#### <p align=\"center\" style=\"color:rgb(55, 113, 197);\">3.2.1&nbsp;Randomly rotating the input point cloud </p>\n",
    "#### <p align=\"center\" style=\"color:rgb(42, 88, 153);\">![GIF](./output/seg/accuracy_vs_rotation.png)</p>\n",
    "\n",
    "#### <p align=\"center\" style=\"color:rgb(55, 113, 197);\">3.2.2&nbsp;Varying the number of points in the input point cloud </p>\n",
    "#### <p align=\"center\" style=\"color:rgb(42, 88, 153);\">![GIF](./output/seg/accuracy_vs_npts.png)</p>\n",
    "\n",
    "#### <p align=\"center\" style=\"color:rgb(55, 113, 197);\"> Exp1. Randomly rotating the input point cloud   </p>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "PointNet relies on a mini-network called the T-Net (Transformation Network) to learn an optimal spatial transformer that aligns input point clouds into a canonical, stable orientation before processing through the rest of the network. This transformation is crucial for achieving rotation invariance. I did not implement this. Thus, PointNet cannot correctly align randomly rotated point clouds. As a result, the same object in different orientations might be interpreted as different by the network because the features extracted will vary with the orientation, leading to a drop in accuracy as we go away from the training data distribution (see plots).  \n",
    "\n",
    "The spikes can be in accuracy with certain large rotations can be explained as such - the model might have learned specific patterns that coincidentally align well with the point cloud at certain rotations; and some objects may have symmetrical features that are more easily recognizable at certain angles, even without proper rotational invariance. This would cause spikes in accuracy at angles where the symmetry is more pronounced and aligns with the model's learned features.\n",
    "</div>\n",
    "\n",
    "#### <p align=\"center\" style=\"color:rgb(55, 113, 197);\"> Exp2. Varying number of points  </p>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "Handling of Increasing Point Counts:\n",
    "PointNet uses a global max pooling layer to aggregate features across all points, which produces a global feature descriptor. While effective for summarization, this approach has a bottleneck effect: beyond a certain number of points (about 2048 in typical implementations), adding more points does not significantly enhance the global descriptor due to the nature of max pooling. The operation simply picks the maximum value across the features, so once the most significant features are captured, additional points do not contribute new information.\n",
    "\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p align=\"center\" style=\"color:rgb(55, 113, 197);\">4.&nbsp;Exploring other architectures </p>\n",
    "\n",
    "\n",
    "\n",
    "### <p align=\"center\" style=\"color:rgb(55, 113, 197);\">4.1.&nbsp;PointNet++ for classification</p>\n",
    "<div align=\"center\"> To improve upon the provided PointNet implementation with PointNet++ features, I implemented Set Abstraction Layers. These layers are used to downsample point clouds and extract local features. They operate by grouping nearby points and applying PointNet to each group to produce a smaller number of representative points with rich local features. Each instance of SA_Layer processes the input data using a single scale or grouping strategy. I do not use multiple radii or scales being used in any single layer. Thus, the architecture is more aligned with the PointNet++ SSG approach. This hierarchical approach helps PointNet++ to perform better on tasks involving fine-grained patterns and local features, which might be missed by the original PointNet due to its global nature. A full implementation of PointNet++ would include explicit mechanisms for sampling and grouping points (using operations like farthest point sampling and ball query), and it would potentially add feature propagation layers if needed for tasks like segmentation.\n",
    "</div>\n",
    "\n",
    "<p align=\"center\" style=\"color:rgb(55, 113, 197);\">A comparative visualization of the results for the classification task against the vanilla PointNet architecture is shown via confusion matrix below</p>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "| | Vanilla PointNet | PointNet++|\n",
    "|:--------:|:--------:|:--------:|\n",
    "|Test accuracy| 96.53% | 97.79% |\n",
    "|Confusion matrix|![GIF](./output/cls/conf_mat.png) |![GIF](./output/cls_ppp/conf_mat.png) |\n",
    "\n",
    "</div>\n",
    "\n",
    "### <p align=\"center\" style=\"color:rgb(55, 113, 197);\">4.2.&nbsp;PointTransformer for classification</p>\n",
    "\n",
    "<div align=\"center\">\n",
    "To improve the provided PointNet model by incorporating principles from point transformer architectures, we can adapt some key elements from the point transformer literature. The fundamental advancement in point transformers compared to PointNet is the use of self-attention mechanisms which allow the model to weigh the importance of different points differently based on the context provided by other points in the input - which is crucial for capturing complex geometric structures more effectively. In my implmentation I hadd positional encoding, multi-headed self-attention and layer normalization. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Positional Encoding: Adds information about the order of points in the input sequence, which is important when using transformers that are otherwise permutation-invariant.\n",
    "\n",
    "PointTransformerLayer:  A transformer encoder layer that applies self-attention to the point cloud data, allowing the model to learn context-dependent point features.\n",
    "\n",
    "Layer Normalization and Dropout: Used within the transformer for training stability and to prevent overfitting.\n",
    "This model should provide more expressive and context-aware feature learning capabilities compared to the vanilla PointNet\n",
    "\n",
    "I also accumulated gradients across mini-batches due to compute contraints and would only update the optimizer at certain points such that other hyperparameters whilst training were kept as the same to the vanilla pointnet module for uniform comparison. However, I was only able to train it for 20 epochs (of batch size 32) in 24 hours on a V100 GPU. I believe that, if allowed to train for the full 250 epochs as the vanilla PointNet, I will obeserve a significant improvement.\n",
    "\n",
    "</div>\n",
    "\n",
    "<p align=\"center\" style=\"color:rgb(55, 113, 197);\">A comparative visualization of the results for the classification task against the vanilla PointNet architecture is shown via confusion matrix below</p>\n",
    "\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "| | Vanilla PointNet | PointTransformer|\n",
    "|:--------:|:--------:|:--------:|\n",
    "|Test accuracy| 96.53% | 93.49% |\n",
    "|Confusion matrix|![GIF](./output/cls/conf_mat.png) |![GIF](./output/cls_tra/conf_mat.png) |\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "#### <p align=\"center\" style=\"color:rgb(42, 88, 153);\">![GIF](./output/cls_tra/accuracy_vs_npts.png)</p>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "Point Transformer employs an attention mechanism, which allows it to weigh and integrate features from each point dynamically. This approach is more sensitive to the contributions of each individual point. As the number of points increases, the transformer has more fine-grained data to work with, enabling better local and global contextual understanding. Consequently, its performance can continue to improve as more points are added, unlike PointNet where the performance plateauss ~2048 points due to global max pooling (critical set of points).\n",
    "\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <p align=\"center\" style=\"color:rgb(55, 113, 197);\">Late days used</p>\n",
    "\n",
    "## <p align=\"center\" style=\"color:rgb(42, 88, 153);\">![One](./output/one.png)</p>\n",
    "<p align=\"center\" style=\"color:rgb(55, 113, 197);\">\n",
    "    <a href=\"https://github.com/learning3d/learning3d.github.io/tree/main/spring23/data/late_days\">Image Source</a> \n",
    "</p>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
